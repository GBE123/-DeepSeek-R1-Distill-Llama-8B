# -*- coding: utf-8 -*-
"""
@author: gbe 2025.03.02
"""

# ç¯å¢ƒè¦æ±‚ï¼šPython 3.10+, PyTorch 2.1+
# å¿…éœ€å®‰è£…åŒ…ï¼š
# pip install -Uqqq bitsandbytes==0.41.3
# pip install -Uqqq transformers==4.38.2 peft==0.9.0 accelerate==0.27.2 trl==0.7.11 datasets
# -*- coding: utf-8 -*-
#pip install -U git+https://github.com/huggingface/trl
#pip install tensorboard
import torch
from datasets import load_dataset
from datasets import IterableDataset
from peft import LoraConfig, prepare_model_for_kbit_training
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    TrainerCallback
)
from trl import SFTTrainer
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training  # æ–°å¢å…³é”®å¯¼å…¥
)
from transformers import DataCollatorForLanguageModeling
from datasets import Dataset
import os
import gradio as gr
import json
import socket
import webbrowser
import psutil
import torch.cuda as cuda  
import platform  
from transformers import TextStreamer
from peft import PeftModel, PeftConfig
import shutil
import os
import time

os.environ["PYTHONUTF8"] = "1"
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# ================== é…ç½®åŒº ==================
#MODEL_NAME = "./DeepSeek-R1-Distill-Llama-8B"
#OUTPUT_DIR = "./deepseek-14b-qlora-finetune"
LOG_DIR = "./logs"


# é‡åŒ–é…ç½®ï¼ˆ4ä½ç²¾åº¦ï¼‰
def get_bnb_config():
    return BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

# QLoRAé…ç½®ï¼ˆæ˜¾å­˜ä¼˜åŒ–ç‰ˆï¼‰
def get_lora_config():
    return LoraConfig(
        r=32,
        lora_alpha=64,
        target_modules=[
            "q_proj",    # æ³¨æ„åŠ›æœºåˆ¶æŸ¥è¯¢æŠ•å½±
            "k_proj",    # æ³¨æ„åŠ›æœºåˆ¶é”®æŠ•å½±
            "v_proj",    # æ³¨æ„åŠ›æœºåˆ¶å€¼æŠ•å½±
            "o_proj",    # æ³¨æ„åŠ›è¾“å‡ºæŠ•å½±
            "gate_proj", # MLPé—¨æ§æŠ•å½± (åŸw1)
            "up_proj"    # MLPä¸ŠæŠ•å½± (åŸw2)
        ],
        modules_to_save=["norm"],  # å…³é”®è°ƒæ•´, "embed_tokens"
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM"
    )

# è®­ç»ƒå‚æ•°ï¼ˆ16GBæ˜¾å­˜ä¼˜åŒ–ï¼‰
def get_training_args(TMP_DIR,num_train_epochs, per_device_train_batch_size,  learning_rate):
    return TrainingArguments(
        output_dir=TMP_DIR,
        num_train_epochs=int(num_train_epochs),
        per_device_train_batch_size=int(per_device_train_batch_size),
        gradient_accumulation_steps=32,
        gradient_checkpointing=True,
        learning_rate=float(learning_rate),
        #learning_rate=2e-5,
        optim="paged_adamw_8bit",
        fp16=True,
        logging_dir=LOG_DIR,
        logging_steps=20,
        save_strategy="steps",
        save_steps=300,
        report_to="tensorboard",
        warmup_ratio=0.1,  # å»¶é•¿warmup
        weight_decay=0.01,  # å¢åŠ æ­£åˆ™åŒ–
        max_grad_norm=0.3,
        lr_scheduler_type="cosine",
        warmup_steps=100
    )

class ConversationProcessor(AutoTokenizer):
    def __init__(self, tokenizer: AutoTokenizer, max_length=256):
        self.tokenizer = tokenizer
        self.max_length = max_length

        # é…ç½®Llamaä¸“ç”¨æ¨¡æ¿
        self.tokenizer.chat_template = """
<|user|>{prompt}<|assistant|>
"""

        # ç¡®ä¿tokenizerçš„pad_tokenæ­£ç¡®è®¾ç½®
        self.tokenizer.pad_token = self.tokenizer.eos_token

    def process_example(self, example):
        try:
            if not isinstance(example, dict):
                print(f"é”™è¯¯ï¼šæ¥æ”¶åˆ°éå­—å…¸ç±»å‹çš„æ ·æœ¬ã€‚æ ·æœ¬ç±»å‹: {type(example)}ï¼Œæ ·æœ¬å†…å®¹: {example}")
                return {"input_ids": [], "attention_mask": []}

            assert "messages" in example, "ç¼ºå¤±messageså­—æ®µ"
            messages = example["messages"]
            assert isinstance(messages, list), "messageså¿…é¡»æ˜¯åˆ—è¡¨"

            # ä»…ä¿ç•™userå’Œassistantè§’è‰²
            filtered_messages = [
                msg for msg in messages
                if msg.get("role") in ["user", "assistant"]
            ]

            # æ ¼å¼åŒ–å¯¹è¯å†…å®¹
            text = ""
            for msg in filtered_messages:
                role = msg["role"]
                content = msg["content"].strip()
                text += f"<|{role}|>{content}"

            # æ·»åŠ ç”Ÿæˆæç¤º
            text += "<|assistant|>"

            # åˆ†è¯
            tokenized = self.tokenizer(
                text,
                truncation=True,
                max_length=self.max_length,
                return_overflowing_tokens=False,
                padding=False
            )
            # è½¬æ¢ä¸ºtorch.Tensor
            input_ids = torch.tensor(tokenized["input_ids"], dtype=torch.long)
            attention_mask = torch.tensor(tokenized["attention_mask"], dtype=torch.long)
            
            # ç”Ÿæˆlabels
            input_ids = tokenized["input_ids"]
            labels = input_ids.copy()
            labels[:-1] = input_ids[1:]
            labels[-1] = -100
            
            return {
                "input_ids": input_ids,
                "attention_mask": attention_mask,
                "labels": labels  # å¿…é¡»åŒ…å«labels
            }
        except Exception as e:
            print(f"æ•°æ®å¤„ç†å¤±è´¥: {e}\næ ·æœ¬: {example}")
            return {"input_ids": [], "attention_mask": []}

# ================== è®­ç»ƒå›è°ƒ ==================
class MemoryOptimizationCallback(TrainerCallback):
    def on_step_end(self, args, state, control, **kwargs):
        torch.cuda.empty_cache()

class CustomDataCollator(DataCollatorForLanguageModeling):
    def __call__(self, examples):
        batch = super().__call__(examples)
        # å®ç°åŠ¨æ€æ‰“åŒ…é€»è¾‘
        input_ids = [seq for ex in examples for seq in self._pack_sequences(ex["input_ids"])]
        attention_mask = [seq for ex in examples for seq in self._pack_sequences(ex["attention_mask"])]
        # è½¬æ¢ä¸ºtorch.Tensor
        batch["input_ids"] = torch.tensor(input_ids, dtype=torch.long)
        batch["attention_mask"] = torch.tensor(attention_mask, dtype=torch.long)
        return batch

    def _pack_sequences(self, input_ids):
        max_len = self.tokenizer.model_max_length
        return [input_ids[i:i+max_len] for i in range(0, len(input_ids), max_len)]

# åŠ è½½æ•°æ®é›†ï¼ˆæµå¼ï¼‰
def get_streaming_dataset(path):
    raw_data = load_dataset("json", data_files=path, streaming=True, encoding="utf-8")["train"]
    return raw_data

# æ•°æ®è´¨é‡æ£€æŸ¥
def validate_sample(sample):
    try:
        assert "messages" in sample, "ç¼ºå¤±å¯¹è¯å­—æ®µ"
        convs = sample.get("messages")
        assert len(convs) >= 1, "å¯¹è¯è½®æ¬¡ä¸è¶³"

        content = " ".join([msg["content"] for msg in sample["messages"]])
        #if len(content.split()) < 15:  # è¿‡æ»¤è¿‡çŸ­æ ·æœ¬
        #    return False
        #if "è¯·è¯¦ç»†æ¨å¯¼" in content and "ç”¨æˆ·è¡¥å……é—®é¢˜" in content: # è¿‡æ»¤å«å¾ªç¯æ¨¡å¼çš„æ ·æœ¬
        #    return False

        return True
    except AssertionError as e:
        print(f"æ— æ•ˆæ ·æœ¬: {sample}\né”™è¯¯: {str(e)}")
        return False

# æµå¼å¤„ç†æ•°æ®é›†
def process_dataset_streaming(dataset_generator, processor):
    for example in dataset_generator:
        if validate_sample(example):
            processed_example = processor.process_example(example)
            yield processed_example


# ================== æ¨ç†æµ‹è¯• ==================
# æ„é€ å¯¹è¯æ¶ˆæ¯
test_messages = [
    {"role": "user", "content": "ä½ æ˜¯ä¸€åèµ„æ·±è¯åˆ¸åˆ†æå¸ˆï¼Œå¦‚ä½•è®¡ç®—åŠ¨æ€å¸‚ç›ˆç‡ï¼Ÿ"}
]


generation_config = {
    "max_new_tokens": 2048,  # å¢åŠ ç”Ÿæˆé•¿åº¦
    "temperature": 0.9,      # æé«˜åˆ›é€ æ€§
    "top_k": 50,             # å¢åŠ é‡‡æ ·å¤šæ ·æ€§
    "repetition_penalty": 1.3,
    "no_repeat_ngram_size": 4,  # ç¦æ­¢4-gramé‡å¤
    "do_sample": True,
    "num_beams": 2,          # ä½¿ç”¨beam search
    "early_stopping": True,
    "eos_token_id": None,  # æ˜¾å¼æŒ‡å®šç»“æŸç¬¦
    "pad_token_id": None
}
from transformers import DataCollatorForSeq2Seq
def get_data_collator(tokenizer):
    return DataCollatorForSeq2Seq(
        tokenizer,
        pad_to_multiple_of=8,
        padding=True,
        return_tensors="pt"
    )

def safe_release(obj):
    """å®‰å…¨é‡Šæ”¾å¯¹è±¡å†…å­˜"""
    if isinstance(obj, torch.nn.Module):
        # PyTorchæ¨¡å‹ä¸“ç”¨æ¸…ç†
        obj.to('cpu')  # å…ˆè½¬ç§»åˆ°CPU
        obj.zero_grad(set_to_none=True)
        for name, child in obj.named_children():
            safe_release(child)
        del obj
    elif hasattr(obj, 'cpu'):
        # å¼ é‡è½¬ç§»åˆ°CPUååˆ é™¤
        obj.cpu()
        del obj
    else:
        del obj
    gc.collect()
    torch.cuda.empty_cache()
import ctypes
def full_memory_purge():
    """å…¨æ ˆå¼å†…å­˜æ¸…ç†"""
    # PyTorchæ¸…ç†
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    # Pythonåƒåœ¾å›æ”¶
    gc.collect()
    # ç³»ç»Ÿçº§å†…å­˜æ•´ç†ï¼ˆLinuxæœ‰æ•ˆï¼‰
    if os.name == 'posix':
        ctypes.CDLL("libc.so.6").malloc_trim(0)
    # Windowsç³»ç»Ÿæ¸…ç†
    elif os.name == 'nt':
        ctypes.windll.kernel32.SetProcessWorkingSetSize(-1, ctypes.c_size_t(1024**3), ctypes.c_size_t(1024**3))
def tensor_obliteration():
    """æ ¸å¼¹çº§å¼ é‡å†…å­˜æ¸…é™¤"""
    for obj in gc.get_objects():
        try:
            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):
                if obj.is_cuda:
                    obj.data = obj.cpu().data
                del obj
        except:
            pass
    full_memory_purge()

        
def fn_train(data_file_path,MODEL_NAME,Y_model_path,tmp_model_path,new_model_path,epochs,batch_size,rate):
    #åŠ è½½æ•°æ®æ–‡ä»¶
    #å‚æ•°æŒ‡å®š
    #å¼€å§‹è®­ç»ƒ
    #ä¿å­˜ç»“æœ
    #åˆå¹¶æ¨¡å‹ 
    try:
        print("å¼€å§‹")
        outstr =""
        outstr = f"åŠ è½½æ•°æ®é›†,åˆ†è¯å¤„ç†....\n"
        yield gr.update(value=outstr)
    
        
        DATASET_PATH = data_file_path.name
        print(DATASET_PATH)
        
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True
        )
        tokenizer.pad_token = tokenizer.eos_token
    
        processor = ConversationProcessor(tokenizer)
    
        dataset_generator = get_streaming_dataset(DATASET_PATH)
        
        processed_dataset = process_dataset_streaming(dataset_generator, processor)
        # å°†ç”Ÿæˆå™¨çš„æ•°æ®æ”¶é›†åˆ°åˆ—è¡¨ä¸­
        data_list = []
        for data in processed_dataset:
            data_list.append(data)
        
        # ä½¿ç”¨ from_list åˆ›å»º Dataset å¯¹è±¡
        train_dataset = Dataset.from_list(data_list)
    

        #train_dataset = IterableDataset.from_generator(
        #    lambda: process_dataset_streaming(dataset_generator, processor)
        #)

        bnb_config = get_bnb_config()
        lora_config = get_lora_config()
        training_args = get_training_args(tmp_model_path,epochs, batch_size, rate)
       
        outstr =  f"åŠ è½½åŸºç¡€æ¨¡å‹....\n"
        yield gr.update(value=outstr)
        
        #print(MODEL_NAME)
        #print(training_args)
        #print(bnb_config)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            quantization_config=bnb_config,
            trust_remote_code=True
        )
        
        outstr =  f"å‡†å¤‡æ¨¡å‹è¿›è¡Œé‡åŒ–4è®­ç»ƒ....\n"
        yield gr.update(value=outstr)
        # å‡†å¤‡æ¨¡å‹è¿›è¡Œ4ä½è®­ç»ƒ
        model = prepare_model_for_kbit_training(model)
    
        # åº”ç”¨LoRAé…ç½®
        model = get_peft_model(model, lora_config)
    
        # å®ä¾‹åŒ–æ•°æ®æ”¶é›†å™¨
        data_collator = CustomDataCollator(tokenizer=tokenizer, mlm=False)
        
        
        outstr =  f"å®ä¾‹åŒ–SFTTrainer....\n"
        yield gr.update(value=outstr)
        # å®ä¾‹åŒ–SFTTrainer
        trainer = SFTTrainer(
            model=model,
            train_dataset=train_dataset,
            args=training_args,
            data_collator=get_data_collator(tokenizer),#data_collator,
            callbacks=[MemoryOptimizationCallback()]
        )
    
              
        outstr =  f"å¼€å§‹è®­ç»ƒ....\n"
        yield gr.update(value=outstr)
        # å¼€å§‹è®­ç»ƒ
        print(f"å¼€å§‹è®­ç»ƒ....\n")
        trainer.train()
        print(f"è®­ç»ƒå®Œæˆ....\n")
        outstr =  f"è®­ç»ƒå®Œæˆ!\n"
        yield gr.update(value=outstr)
        
        # ä¿å­˜æ¨¡å‹å¢é‡     
        trainer.save_model(new_model_path)
        tokenizer.save_pretrained(new_model_path)
    
        outstr =  f"è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜æ¨¡å‹å¢é‡...\n"
        print(f"è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜\n")
        
        yield gr.update(value=outstr)
        
        #é‡Šæ”¾èµ„æºä»¥ä¾¿è¿›è¡Œåˆå¹¶
        del trainer
        del model
        del train_dataset        
        #del data_list
        torch.cuda.empty_cache()
        gc.collect()  

        time.sleep(3)
        yield gr.update(value=outstr)
        
        tmp_mod_path = "./"+tmp_model_path+"/checkpoint-5"
        
        #åˆå¹¶æ¨¡å‹        
        peft_config = PeftConfig.from_pretrained(tmp_mod_path, trust_remote_code=True)
        
        outstr = outstr.join(f"åŠ è½½åŸºç¡€æ¨¡å‹\n")
        yield gr.update(value=outstr)
        
        print(f"åŠ è½½åŸºç¡€æ¨¡å‹\n")    
        # åŠ è½½åŸºç¡€æ¨¡å‹æ—¶æŒ‡å®šåŠç²¾åº¦
        base_model = AutoModelForCausalLM.from_pretrained(
            peft_config.base_model_name_or_path,
            return_dict=True,
            device_map="cpu",
            torch_dtype=torch.float16  # å…³é”®ä¿®æ”¹ï¼šä¿æŒåŠç²¾åº¦
        )
        outstr = outstr.join(f"åŠ è½½å¢é‡æ¨¡å‹æ¨¡å‹\n")
        yield gr.update(value=outstr)
        print(f"åŠ è½½å¢é‡æ¨¡å‹æ¨¡å‹\n")
        # åŠ è½½å¹¶åˆå¹¶LoRAé€‚é…å™¨
        
        model = PeftModel.from_pretrained(base_model, tmp_mod_path)
        merged_model = model.merge_and_unload()

        outstr = outstr.join(f"åˆ›å»ºç›®å½•\n")
        yield gr.update(value=outstr)
        shutil.rmtree(new_model_path)
        os.mkdir(new_model_path)
        outstr = outstr.join(f"ä¿å­˜ç›®å½•\n")
        yield gr.update(value=outstr)
        print(f"ä¿å­˜æ—¶ä¿æŒåŠç²¾åº¦å¹¶æ¸…ç†å­˜å‚¨æ ¼å¼\n")
        # ä¿å­˜æ—¶ä¿æŒåŠç²¾åº¦å¹¶æ¸…ç†å­˜å‚¨æ ¼å¼
        merged_model.save_pretrained(
            new_model_path,
            safe_serialization=True,  # ä½¿ç”¨safetensorsæ ¼å¼
            max_shard_size="2GB"      # åˆ†ç‰‡å­˜å‚¨
        )
        print(f"åŒæ—¶ä¿å­˜tokenizer\n")
        # åŒæ—¶ä¿å­˜tokenizer
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        tokenizer.save_pretrained(new_model_path)
        outstr = outstr.join(f"ä¿å­˜æœ€æ–°å¾®è°ƒæ¨¡å‹å®Œæ¯•\n")
        yield gr.update(value=outstr)
        
    
        del base_model, model, merged_model
        #tensor_obliteration()
        #full_memory_purge()
        # å¤šçº§GC
        for _ in range(3):
            gc.collect()
            torch.cuda.empty_cache()
        outstr = "å…¨éƒ¨å¾®è°ƒå·¥ä½œå®Œæˆ...100%\n"
        print(f"å…¨éƒ¨å¾®è°ƒå·¥ä½œå®Œæˆ...100%\n")
        return outstr
    except Exception as e:
        # å¤„ç†å¼‚å¸¸æƒ…å†µ
        error_str = f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}\n"
        outstr =  f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}\n"
        print(error_str)
        yield gr.update(value=outstr)
        return error_str
    finally:
        # é‡Šæ”¾èµ„æº
        # åˆ é™¤æ¨¡å‹å’Œå¼ é‡å¯¹è±¡
        # è¿™é‡Œç”±äºä¹‹å‰æ²¡æœ‰å®é™…çš„æ¨¡å‹å¯¹è±¡åˆ›å»ºï¼Œåªæ˜¯æ¨¡æ‹Ÿï¼Œæ‰€ä»¥æ³¨é‡Šæ‰ç›¸å…³ä»£ç 
        # del model

        if 'trainer' in locals():
            del trainer
        if 'model' in locals():
            del model
        if 'train_dataset' in locals():
            del train_dataset
        if 'data_list' in locals():
            del data_list
        # é‡Šæ”¾ GPU ç¼“å­˜        
        torch.cuda.empty_cache()

        # åˆ é™¤æ•°æ®é›†å¯¹è±¡
        # ç”±äºä¹‹å‰æ²¡æœ‰å®é™…çš„æ•°æ®é›†å¯¹è±¡ï¼Œè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨æ—¶æŒ‰éœ€åˆ é™¤
        # del train_dataset

        # è§¦å‘ Python åƒåœ¾å›æ”¶
        gc.collect()
    return f"å…¨éƒ¨å¾®è°ƒå·¥ä½œå®Œæˆ...100%\n"

g_load_model_flag=False
g_model_radio=""
g_ask_model=None
g_ask_tokenizer=None
def fn_ask(radio,MODEL_NAME,Y_model_path,new_model_path,question_input):
    #åŠ è½½æ¨¡å‹
    #æ¨¡å‹ç”Ÿæˆ
    #è¾“å‡ºç»“æœ
    try:
        if g_load_model_flag==False:
            # æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„
            model_name=Y_model_path
            print(radio)
            if radio=="å¾®è°ƒåæ¨¡å‹":
                model_name = new_model_path  # æ›¿æ¢ä¸ºå®é™…çš„æ¨¡å‹è·¯å¾„
            
            print(model_name)
            # åŠ è½½åˆ†è¯å™¨
            g_ask_tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            load_in_4bit = True
            bnb_4bit_compute_dtype = torch.float16
            bnb_4bit_use_double_quant = True
            bnb_4bit_quant_type = "nf4"
            
                   
            # ä½¿ç”¨ int4 é‡åŒ–åŠ è½½æ¨¡å‹
            
            g_ask_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map="auto",
                torch_dtype=bnb_4bit_compute_dtype,
                quantization_config={
                "load_in_4bit": load_in_4bit,
                "bnb_4bit_compute_dtype": bnb_4bit_compute_dtype,
                "bnb_4bit_use_double_quant": bnb_4bit_use_double_quant,
                "bnb_4bit_quant_type": bnb_4bit_quant_type
                }
            )
            # æ‰“å°æ¨¡å‹ç»“æ„
            #print(model)
           
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            
            # æ‰“å°å½“å‰æ˜¾å­˜å ç”¨æƒ…å†µ
            if torch.cuda.is_available():
                print(f"å½“å‰æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB")    
             
        # è¾“å…¥æ–‡æœ¬        
        inputs = g_ask_tokenizer(question_input, return_tensors='pt', padding=True, truncation=True)
        input_ids = inputs['input_ids'].to(device)
        attention_mask = inputs['attention_mask'].to(device)


         
        streamer = TextStreamer(g_ask_tokenizer, skip_prompt=True)  # è·³è¿‡é‡å¤æ˜¾ç¤ºé—®é¢˜
        # ç”Ÿæˆæ–‡æœ¬æ—¶ä½¿ç”¨æµå¼å¤„ç†
        
        generation_kwargs = dict(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=2048,
            do_sample=True,         # å¯ç”¨æŠ½æ ·ç­–ç•¥
            top_p=0.9,              # æ¨èé…åˆä½¿ç”¨æ ¸æŠ½æ ·
            temperature=0.7,        # æ§åˆ¶ç”Ÿæˆéšæœºæ€§
            streamer=streamer,
            num_beams=1,            # å¿…é¡»è®¾ç½®ä¸º1
            eos_token_id=g_ask_tokenizer.eos_token_id
        )
        
        # åœ¨ç”Ÿæˆæ—¶å®æ—¶æµå¼è¾“å‡º
        output = g_ask_model.generate(**generation_kwargs)
        
        generated_text = ""
        for token_id in output[0]:
            new_token = g_ask_tokenizer.decode(token_id.item(), skip_special_tokens=True)            
            generated_text += new_token
            yield generated_text  # ä½¿ç”¨yieldé€æ­¥è¿”å›ç»“æœ
            time.sleep(0.1)  # æ¨¡æ‹Ÿç”Ÿæˆå»¶è¿Ÿ
        
        del output,attention_mask,input_ids,inputs

        # é‡Šæ”¾ GPU ç¼“å­˜        
        #torch.cuda.empty_cache()

        #gc.collect()   

    except Exception as e:
        yield(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
        torch.cuda.empty_cache()
        
    finally:
        # é‡Šæ”¾èµ„æº
        # åˆ é™¤æ¨¡å‹å’Œå¼ é‡å¯¹è±¡
        # è¿™é‡Œç”±äºä¹‹å‰æ²¡æœ‰å®é™…çš„æ¨¡å‹å¯¹è±¡åˆ›å»ºï¼Œåªæ˜¯æ¨¡æ‹Ÿï¼Œæ‰€ä»¥æ³¨é‡Šæ‰ç›¸å…³ä»£ç 
        # del model

        if 'model' in locals():
            del model

        # é‡Šæ”¾ GPU ç¼“å­˜        
        torch.cuda.empty_cache()

        # åˆ é™¤æ•°æ®é›†å¯¹è±¡
        # ç”±äºä¹‹å‰æ²¡æœ‰å®é™…çš„æ•°æ®é›†å¯¹è±¡ï¼Œè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨æ—¶æŒ‰éœ€åˆ é™¤
        # del train_dataset

        # è§¦å‘ Python åƒåœ¾å›æ”¶
        gc.collect()
   
def get_system_info():
    
    cpu_model = torch.cuda.get_device_name(0)

    # è·å–å†…å­˜æ€»é‡ï¼ˆå•ä½ï¼šGBï¼‰
    memory = psutil.virtual_memory()
    memory_total = memory.total / (1024 ** 3)

    # è·å–æ˜¾å­˜æ€»é‡ï¼ˆå•ä½ï¼šGBï¼‰
    if cuda.is_available():
        cuda_total = cuda.get_device_properties(0).total_memory / (1024 ** 3)
    else:
        cuda_total = 0

    return cpu_model, cuda_total, memory_total

# è·å–ç³»ç»Ÿä¿¡æ¯
cpu_model, cuda_total, memory_total = get_system_info()


#================================ç¨‹åºç•Œé¢æ„å»º=============================================================================
# Gradioç•Œé¢
html_head = """
<head>
    <meta charset="UTF-8">
    <!-- å…¶ä»–å¤´éƒ¨ä¿¡æ¯ -->
</head>
"""

with gr.Blocks(
        title="hi,GBEä»Šå¤©æ€ä¹ˆæ ·",       
        css="""
        .gradio-container {max-width: 2000px !important}
        .answer-box {min-height: 500px !important;}
        .left-panel {padding-right: 20px; border-right: 1px solid #eee;}
        .right-panel {height: 100vh;}
        .wide-row { width: 80%; }
        .green-button {
            background-color: green;
            color: white; 
        }
       .gradio-label {
            font-size: 8px !important;
            font-weight: normal !important;
        }
        .gradio-container input {
            font-size: 8px !important;
        }
        .gradio-container textbox {
            font-size: 8px !important;
        }
        .gray-background textarea, .gray-background input[type="text"] {
        background-color: #cccccc !important;
        }        
        """
        ) as demo:
                     
    gr.HTML(html_head, visible=False)
    gr.Markdown("å¼€å§‹å§...")
    with gr.Row(elem_classes="wide-row"):        
        with gr.Column(scale=10, elem_classes="left-panel"):
            gr.Markdown("## ğŸ“ ç³»ç»Ÿä¿¡æ¯åŠå‚æ•°é…ç½®")
            with gr.Group():                
                data_file_input = gr.File(label="ä¸Šä¼ jsonæ–‡æ¡£", file_types=[".json"])
                with gr.Row():
                    cpu_textbox = gr.Textbox(label="æ˜¾å¡",interactive=False,value=f"{cpu_model}",elem_classes="gray-background")#åªè¯»                    
                    xc_textbox = gr.Textbox(label="æ˜¾å­˜",interactive=False, value=f"{cuda_total:.2f} GB",elem_classes="gray-background")#åªè¯»
                    nc_textbox = gr.Textbox(label="å†…å­˜",interactive=False, value=f"{memory_total:.2f} GB",elem_classes="gray-background")#åªè¯»
                    data_file_size_textbox = gr.Textbox(label="æ•°æ®é›†æ–‡ä»¶å¤§å°",interactive=False,elem_classes="gray-background")
                with gr.Row():    
                    MODEL_NAME_textbox = gr.Textbox(label="å¾®è°ƒæ¨¡å‹åç§°", value="DeepSeek-R1-Distill-Llama-8B")#åªè¯»                                     
                    Y_model_path_textbox = gr.Textbox(label="åŸæ¨¡å‹æ–‡ä»¶è·¯å¾„(æ–‡ä»¶å¤¹)", value="DeepSeek-R1-Distill-Llama-8B")                
                with gr.Row():    
                    tmp_path_textbox = gr.Textbox(label="ä¸´æ—¶æ–‡ä»¶è·¯å¾„(æ–‡ä»¶å¤¹)",value="DeepSeek-R1-Distill-Llama-8B-tmp")
                    new_model_path_textbox = gr.Textbox(label="æ–°æ¨¡å‹æ–‡ä»¶è·¯å¾„(æ–‡ä»¶å¤¹)",value="DeepSeek-R1-Distill-Llama-8B-STOCK")
                with gr.Row():    
                    epochs_textbox = gr.Textbox(label="è½®æ¬¡",value="5")#åªè¯»
                    batch_size_textbox = gr.Textbox(label="æ‰¹æ¬¡å¤§å°(æ¯æ¬¡å¤šå°‘æ¡æ•°æ®)",value="8")#åªè¯»
                    rate_textbox = gr.Textbox(label="å­¦ä¹ ç‡",value="2e-5")#åªè¯»                
                
                train_btn = gr.Button("å¼€å§‹å¾®è°ƒè®­ç»ƒ", variant="primary")
            gr.Markdown("## â“ è®­ç»ƒ&é—®ç­”")
            with gr.Group():
                options = ["åŸæ¨¡å‹", "å¾®è°ƒåæ¨¡å‹"]
                radio = gr.Radio(choices=options, label="è¯·é€‰æ‹©æ¨¡å‹",value="å¾®è°ƒåæ¨¡å‹")
                question_input = gr.Textbox(
                    label="è¾“å…¥é—®é¢˜",
                    lines=4,
                    placeholder="ä¾‹å¦‚ï¼šæœ¬æ–‡æ¡£çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                    elem_id="question-input"
                )
                ask_btn = gr.Button("ğŸ” å¼€å§‹æé—®", variant="primary",  elem_classes="green-button")
                status_display = gr.HTML("", elem_id="status-display")
        with gr.Column(scale=10, elem_classes="right-panel"):
            gr.Markdown("## ğŸ“ è®­ç»ƒæƒ…å†µ")
            train_output = gr.Textbox(
                label="è®­ç»ƒæƒ…å†µ",
                lines=4,
                placeholder="ä¾‹å¦‚ï¼šæœ¬æ–‡æ¡£çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                elem_id="train-output"
            )
            gr.Markdown("## ğŸ“ ç­”æ¡ˆ")
            answer_output = gr.Textbox(
                label="å›ç­”",
                interactive=False,
                lines=25,
                elem_classes="answer-box",
                autoscroll=True,
                show_copy_button=True
            )
            gr.Markdown("""
            <div class="footer-note">
                *å›ç­”ç”Ÿæˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…<br>
                *æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯åŸºäºå‰æ–‡ç»§ç»­æé—®
            </div>
            """)
    train_btn.click(
        fn=fn_train,
        inputs=[data_file_input,MODEL_NAME_textbox,Y_model_path_textbox,tmp_path_textbox,new_model_path_textbox,epochs_textbox,
                batch_size_textbox,rate_textbox],
        outputs=[train_output],
        show_progress="hidden"
    )
    ask_btn.click(
        fn=fn_ask,
        inputs=[radio,MODEL_NAME_textbox,Y_model_path_textbox,new_model_path_textbox,question_input],
        outputs=[answer_output],
        show_progress="hidden"
    )


def is_port_available(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        return s.connect_ex(('127.0.0.1', port)) != 0  # æ›´å¯é çš„æ£€æµ‹æ–¹å¼
if __name__ == "__main__":    
    ports = [17995, 17996, 17997, 17998, 17999]
    selected_port = next((p for p in ports if is_port_available(p)), None)
    
    if not selected_port:
        print("æ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œè¯·æ‰‹åŠ¨é‡Šæ”¾ç«¯å£")
        exit(1)
        
    try:    
        webbrowser.open(f"http://127.0.0.1:{selected_port}")
        demo.launch(
            server_port=selected_port,
            server_name="0.0.0.0",
            show_error=True,
            ssl_verify=False
        )
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {str(e)}")
# ================== é‡Šæ”¾èµ„æº ==================
# åˆ é™¤æ¨¡å‹å’Œå¼ é‡å¯¹è±¡
#del model, inputs, outputs
del g_ask_model
# é‡Šæ”¾ GPU ç¼“å­˜
torch.cuda.empty_cache()
# åˆ é™¤æ•°æ®é›†å¯¹è±¡
# è§¦å‘ Python åƒåœ¾å›æ”¶
import gc
gc.collect()
